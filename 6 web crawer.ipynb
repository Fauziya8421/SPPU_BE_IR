{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "381b65f4-1a93-4a3e-b290-ba6adfa4ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests  # For sending HTTP requests\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import pandas as pd  # For data manipulation and creating DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "215e324a-f1da-4fe0-969e-29a539b0cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to scrape data from the provided URL\n",
    "def scrape(url):\n",
    "    # Define headers to simulate a browser request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the specified URL with headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Parse the response text into an HTML format using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Initialize empty lists to store product details\n",
    "    products = []\n",
    "    prices = []\n",
    "    ratings = []\n",
    "    images = []\n",
    "\n",
    "    # Find all product names using their specific HTML class and append text to products list\n",
    "    p_name = soup.find_all('a', class_=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\")\n",
    "    for i in p_name:\n",
    "        products.append(i.text)\n",
    "\n",
    "    # Find all product prices using their specific HTML class and append text to prices list\n",
    "    p_price = soup.find_all('span', class_=\"a-price-whole\")\n",
    "    for i in p_price:\n",
    "        prices.append(i.text)\n",
    "\n",
    "    # Find all product ratings using their specific HTML class and append text to ratings list\n",
    "    p_rating = soup.find_all('span', class_=\"a-size-base s-underline-text\")\n",
    "    for i in p_rating:\n",
    "        ratings.append(i.text)\n",
    "\n",
    "    # Find all image elements and get their 'src' attribute to get the image URL\n",
    "    p_images = soup.find_all('img')\n",
    "    k = 0  # Counter to limit the number of images saved\n",
    "    for i in p_images:\n",
    "        images.append(i.get('src'))\n",
    "        k += 1\n",
    "        # Stop collecting images after 20 entries to limit the data\n",
    "        if k == 20:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Ensure all lists have the same length by padding with 'N/A' for missing values\n",
    "    max_len = max(len(products), len(prices), len(ratings), len(images))\n",
    "    products.extend(['N/A'] * (max_len - len(products)))\n",
    "    prices.extend(['N/A'] * (max_len - len(prices)))\n",
    "    ratings.extend(['N/A'] * (max_len - len(ratings)))\n",
    "    images.extend(['N/A'] * (max_len - len(images)))\n",
    "\n",
    "\n",
    "    # Create a DataFrame from the collected product data\n",
    "    df = pd.DataFrame({\n",
    "        'Product': products,\n",
    "        'Price': prices,\n",
    "        'Rating': ratings,\n",
    "        'Image_url': images\n",
    "    })\n",
    "\n",
    "    return df  # Return the DataFrame with scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "11cd5cd5-5a08-4467-adf7-27e67a37c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from the specified Amazon page URL\n",
    "df = scrape(\"https://www.amazon.in/s?k=mobile+phone+under+30000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c05b957b-448e-4d45-ac59-5d67cf98f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scraped data to a CSV file named 'amazon_data.csv'\n",
    "df.to_csv('filp_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "44748872-c405-4e5a-80ea-f2c15478f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory named 'amazon' to store product images\n",
    "import os\n",
    "os.makedirs('flip', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b45aec77-8e48-4a9f-8921-8ab73a947fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each image URL in the DataFrame and save each image\n",
    "for i, url in enumerate(df['Image_url']):\n",
    "    try:\n",
    "        # Send a GET request to each image URL to fetch image data\n",
    "        response = requests.get(url)\n",
    "        # Write the image data to a file in the 'amazon' directory\n",
    "        with open(f\"flip/{i}.jpg\", 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    except:\n",
    "        pass  # Ignore any errors in downloading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586bb4a-3784-46e4-83c3-1b7e662933b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330c424-cc85-4cc7-af21-dd29799631c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
